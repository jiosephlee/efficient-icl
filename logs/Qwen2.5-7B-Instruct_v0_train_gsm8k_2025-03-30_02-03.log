2025-03-30 02:03:29,556 - INFO - Starting experiment: Qwen2.5-7B-Instruct_v0_train_gsm8k_2025-03-30_02-03
2025-03-30 02:03:29,556 - INFO - Arguments: Namespace(model='Qwen/Qwen2.5-7B-Instruct', mode='train', lora_name='v0', dataset='gsm8k', checkpoint_for_continued_training=None)
2025-03-30 02:03:29,556 - INFO - Using config: v0
2025-03-30 02:03:29,556 - INFO - Using max_seq_length=2048, lora_rank=32
2025-03-30 02:04:10,298 - INFO - Model loaded: Qwen/Qwen2.5-7B-Instruct
2025-03-30 02:04:13,500 - INFO - PEFT model configured
2025-03-30 02:04:13,500 - INFO - Loading gsm8k training dataset
2025-03-30 02:04:14,517 - INFO - Dataset loaded with 7473 examples
2025-03-30 02:04:14,558 - INFO - Training configuration: UnslothGRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.99,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.04,
bf16=False,
bf16_full_eval=False,
data_seed=3407,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
eval_accumulation_steps=2,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/Qwen2.5-7B-Instruct/v0/runs/Mar30_02-04-14_ShenLab,
logging_first_step=False,
logging_nan_inf_filter=False,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_completion_length=952,
max_grad_norm=0.1,
max_prompt_length=1096,
max_steps=250,
metric_for_best_model=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_generations=8,
num_train_epochs=3.0,
optim=OptimizerNames.PAGED_ADAMW_8BIT,
optim_args=None,
optim_target_modules=None,
output_dir=./checkpoints/Qwen2.5-7B-Instruct/v0/,
overwrite_output_dir=None,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.9,
ref_model_sync_steps=64,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=./checkpoints/Qwen2.5-7B-Instruct/v0/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=250,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=3407,
skip_memory_metrics=True,
split_batches=None,
sync_ref_model=False,
temperature=0.9,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=250,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
unsloth_num_chunks=-1,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_vllm=False,
vllm_device=auto,
vllm_dtype=auto,
vllm_gpu_memory_utilization=0.9,
vllm_max_model_len=None,
vllm_sampling_params=None,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.1,
)
2025-03-30 02:04:14,638 - INFO - Starting training
2025-03-30 04:08:37,506 - INFO - Training completed
2025-03-30 04:08:37,506 - INFO - Saving LoRA adapter to ./models/Qwen2.5-7B-Instruct/v0/
2025-03-30 04:08:42,780 - INFO - Output without LoRA:
2025-03-30 04:08:42,780 - INFO - Calculating the exact value of pi is impossible due to its nature as an irrational number, but we can approximate it to any degree of accuracy. A common method to approximate pi is through the Leibniz formula for Ï€, which is an infinite series:

\[
\pi = 4 \times \left(1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} - \cdots \right)
\]

However, this series converges very slowly, meaning we would need many terms to get an accurate approximation. For a more efficient method, we can use the Chudnovsky algorithm, which converges much faster:

\[
\frac{1}{\pi} = 12 \sum_{k=0}^{\infty} \frac{(-1)^k (6k)! (13591409 + 545140134k)}{(3k)! (k!)^3 640320^{3k + 3/2}}
\]

This formula can compute pi to a high degree of accuracy. Using this or similar formulas, we can calculate pi to many decimal places.

For practical purposes, pi is often approximated as 3.14159 or 3.14 depending on the precision needed. More precise values are used in scientific and engineering calculations.

<answer>
Pi is an irrational number, but it can be approximated as 3.14159 for most practical purposes. For higher precision, the Chudnovsky algorithm or similar high-precision methods are used.
</answer>
2025-03-30 04:08:45,591 - INFO - Output with LoRA:
2025-03-30 04:08:45,591 - INFO - Calculating pi to its exact value is not possible with current computational methods because pi is an irrational number, meaning it has an infinite number of non-repeating digits. However, we can approximate pi to a high degree of accuracy.
<reasoning>
To provide a commonly used approximation, pi is often taken as 3.14159. For more precision, it can be approximated as 3.141592653589793.
</reasoning>
<answer>
A common approximation for pi is 3.14159, but the exact value is an infinite non-repeating sequence of digits starting from 3.141592653589793...
</answer>

2025-03-30 04:08:45,591 - INFO - Loading gsm8k test dataset for evaluation
2025-03-30 04:08:45,591 - INFO - Running zero-shot evaluation
2025-03-30 04:08:46,093 - INFO - Test dataset loaded with 1319 examples
2025-03-30 04:08:46,093 - INFO - Starting zero-shot evaluation
2025-03-30 04:08:46,093 - INFO - Using the lora adapter: ./models/Qwen2.5-7B-Instruct/v0/
2025-03-30 05:14:56,270 - INFO - Zero-shot detailed results saved to ././models/Qwen2.5-7B-Instruct/v0//gsm8k_zeroshot_2025-03-30_02-03_results.json
2025-03-30 05:14:56,270 - INFO - Zero-shot Accuracy: 0.84%
2025-03-30 05:14:56,270 - INFO - Running few-shot evaluation
2025-03-30 05:14:57,040 - INFO - Few-shot test dataset loaded with 1319 examples
2025-03-30 05:14:57,040 - INFO - Starting few-shot evaluation
2025-03-30 06:09:49,561 - INFO - Few-shot detailed results saved to ././models/Qwen2.5-7B-Instruct/v0//gsm8k_4-shot_2025-03-30_02-03_results.json
2025-03-30 06:09:49,561 - INFO - Few-shot Accuracy: 0.86%
2025-03-30 06:09:49,561 - INFO - Experiment completed. Log saved to ./logs/Qwen2.5-7B-Instruct_v0_train_gsm8k_2025-03-30_02-03.log
