🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
INFO 03-29 02:28:38 [__init__.py:256] Automatically detected platform cuda.
2025-03-29 02:28:39,622 - INFO - Starting experiment: Qwen2.5-7B-Instruct_v0_evaluate_gsm8k_2025-03-29_02-28
2025-03-29 02:28:39,622 - INFO - Arguments: Namespace(model='Qwen/Qwen2.5-7B-Instruct', mode='evaluate', lora_name='v0', dataset='gsm8k', few_shot=5, checkpoint_for_continued_training=None)
2025-03-29 02:28:39,622 - INFO - Using max_seq_length=2048, lora_rank=32
==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.50.0. vLLM: 0.8.1.
   \\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.586 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 58.68%
Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.59 GB.
Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.
Unsloth: vLLM's KV Cache can use up to 7.86 GB. Also swap space = 6 GB.
INFO 03-29 02:28:45 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.
WARNING 03-29 02:28:45 [arg_utils.py:1765] --quantization bitsandbytes is not supported by the V1 Engine. Falling back to V0. 
Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}
INFO 03-29 02:28:45 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":0,"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":192}, use_cached_outputs=False, 
INFO 03-29 02:28:45 [cuda.py:285] Using Flash Attention backend.
INFO 03-29 02:28:45 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 03-29 02:28:45 [model_runner.py:1110] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...
INFO 03-29 02:28:45 [loader.py:1137] Loading weights with BitsAndBytes quantization. May take a while ...
INFO 03-29 02:28:45 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.03it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.64it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.77it/s]

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.02it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.60it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.72it/s]

INFO 03-29 02:28:48 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 03-29 02:28:48 [model_runner.py:1146] Model loading took 6.8758 GB and 2.676602 seconds
INFO 03-29 02:28:50 [worker.py:267] Memory profiling takes 1.34 seconds
INFO 03-29 02:28:50 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.59GiB) x gpu_memory_utilization (0.59) = 13.84GiB
INFO 03-29 02:28:50 [worker.py:267] model weights take 6.88GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.06GiB; the rest of the memory reserved for KV Cache is 5.85GiB.
INFO 03-29 02:28:50 [executor_base.py:111] # cuda blocks: 6845, # CPU blocks: 7021
INFO 03-29 02:28:50 [executor_base.py:116] Maximum concurrency for 2048 tokens per request: 53.48x
INFO 03-29 02:28:53 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/27 [00:00<?, ?it/s]Capturing CUDA graph shapes:   4%|▎         | 1/27 [00:00<00:16,  1.55it/s]Capturing CUDA graph shapes:   7%|▋         | 2/27 [00:01<00:15,  1.60it/s]Capturing CUDA graph shapes:  11%|█         | 3/27 [00:01<00:14,  1.63it/s]Capturing CUDA graph shapes:  15%|█▍        | 4/27 [00:02<00:13,  1.66it/s]Capturing CUDA graph shapes:  19%|█▊        | 5/27 [00:03<00:13,  1.67it/s]Capturing CUDA graph shapes:  22%|██▏       | 6/27 [00:03<00:12,  1.67it/s]Capturing CUDA graph shapes:  26%|██▌       | 7/27 [00:04<00:11,  1.68it/s]Capturing CUDA graph shapes:  30%|██▉       | 8/27 [00:04<00:11,  1.67it/s]Capturing CUDA graph shapes:  33%|███▎      | 9/27 [00:05<00:10,  1.72it/s]Capturing CUDA graph shapes:  37%|███▋      | 10/27 [00:05<00:09,  1.74it/s]Capturing CUDA graph shapes:  41%|████      | 11/27 [00:06<00:09,  1.75it/s]Capturing CUDA graph shapes:  44%|████▍     | 12/27 [00:07<00:08,  1.72it/s]Capturing CUDA graph shapes:  48%|████▊     | 13/27 [00:07<00:08,  1.72it/s]Capturing CUDA graph shapes:  52%|█████▏    | 14/27 [00:08<00:07,  1.75it/s]Capturing CUDA graph shapes:  56%|█████▌    | 15/27 [00:08<00:06,  1.77it/s]Capturing CUDA graph shapes:  59%|█████▉    | 16/27 [00:09<00:06,  1.79it/s]Capturing CUDA graph shapes:  63%|██████▎   | 17/27 [00:09<00:05,  1.82it/s]Capturing CUDA graph shapes:  67%|██████▋   | 18/27 [00:10<00:04,  1.85it/s]Capturing CUDA graph shapes:  70%|███████   | 19/27 [00:10<00:04,  1.86it/s]Capturing CUDA graph shapes:  74%|███████▍  | 20/27 [00:11<00:03,  1.87it/s]Capturing CUDA graph shapes:  78%|███████▊  | 21/27 [00:11<00:03,  1.88it/s]Capturing CUDA graph shapes:  81%|████████▏ | 22/27 [00:12<00:02,  1.89it/s]Capturing CUDA graph shapes:  85%|████████▌ | 23/27 [00:12<00:02,  1.91it/s]Capturing CUDA graph shapes:  89%|████████▉ | 24/27 [00:13<00:01,  1.90it/s]Capturing CUDA graph shapes:  93%|█████████▎| 25/27 [00:14<00:01,  1.91it/s]Capturing CUDA graph shapes:  96%|█████████▋| 26/27 [00:14<00:00,  1.92it/s]Capturing CUDA graph shapes: 100%|██████████| 27/27 [00:15<00:00,  1.94it/s]Capturing CUDA graph shapes: 100%|██████████| 27/27 [00:15<00:00,  1.79it/s]
INFO 03-29 02:29:08 [model_runner.py:1570] Graph capturing finished in 15 secs, took 0.56 GiB
INFO 03-29 02:29:08 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 20.20 seconds
2025-03-29 02:29:11,407 - INFO - Model loaded: Qwen/Qwen2.5-7B-Instruct
Unsloth 2025.3.18 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
2025-03-29 02:29:14,565 - INFO - PEFT model configured
2025-03-29 02:29:14,565 - INFO - Loading gsm8k test dataset for evaluation
2025-03-29 02:29:14,565 - INFO - Running zero-shot evaluation
2025-03-29 02:29:15,086 - INFO - Test dataset loaded with 1319 examples
2025-03-29 02:29:15,086 - INFO - Starting zero-shot evaluation
2025-03-29 02:29:15,086 - INFO - Using the lora adapter: models/Qwen2.5-7B-Instruct/v0/
  0%|          | 0/1319 [00:00<?, ?it/s]  0%|          | 0/1319 [00:00<?, ?it/s]
2025-03-29 02:29:15,087 - INFO - Running few-shot evaluation
[{'content': '\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n', 'role': 'system'}, {'content': "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?", 'role': 'user'}]
<|im_start|>system

Respond in the following format:
<reasoning>
...
</reasoning>
<answer>
...
</answer>
<|im_end|>
<|im_start|>user
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?<|im_end|>
<|im_start|>assistant

GSM8K Test Accuracy: 0.0000 (0/0)
Map:   0%|          | 0/1319 [00:00<?, ? examples/s]Map:  47%|████▋     | 614/1319 [00:00<00:00, 6114.61 examples/s]Map: 100%|██████████| 1319/1319 [00:00<00:00, 5505.07 examples/s]Map: 100%|██████████| 1319/1319 [00:00<00:00, 5564.89 examples/s]
2025-03-29 02:29:15,848 - INFO - Few-shot test dataset loaded with 1319 examples
2025-03-29 02:29:15,848 - INFO - Starting few-shot evaluation
  0%|          | 0/1319 [00:00<?, ?it/s]  0%|          | 0/1319 [00:00<?, ?it/s]
2025-03-29 02:29:15,849 - INFO - Zero-shot detailed results saved to ./models/Qwen2.5-7B-Instruct/v0//gsm8k_zeroshot_2025-03-29_02-28_results.json
[{'content': '\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n', 'role': 'system'}, {'content': 'Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?', 'role': 'user'}, {'content': '<reasoning>\nMimi has 2 x 12 = <<2*12=24>>24 sea shells.\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\n</reasoning>\n<answer>\n16\n</answer>\n', 'role': 'assistant'}, {'content': "Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?", 'role': 'user'}, {'content': '<reasoning>\nHe has 6 - 2 = <<6-2=4>>4 cats.\nHe has 4 - 1 = <<4-1=3>>3 parrots.\nHe has 4 + 6 = <<4+6=10>>10 snakes.\nHe has a total of 2 + 4 + 3 + 10 = <<2+4+3+10=19>>19 pets.\n</reasoning>\n<answer>\n19\n</answer>\n', 'role': 'assistant'}, {'content': 'Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?', 'role': 'user'}, {'content': "<reasoning>\nDad gave Olaf 10 toy cars,\nMom has given Olaf 5 more toy cars than Dad, so 10 + 5 = <<10+5=15>>15 toy cars\nAuntie gave Olaf 6 toy cars,\nUncle has given 1 less toy than Auntie, so 6 - 1 = <<6-1=5>>5 toy cars\nGrandpa gave Olaf 2 * 5 = <<2*5=10>>10 toy cars.\nAll the family together gave Olaf 10 +15 + 6 + 5 + 10 = <<10+15+6+5+10=46>>46.\nAdding the cars Olaf already had, Olaf's collection has 150 + 46 = <<150+46=196>>196 cars.\n</reasoning>\n<answer>\n196\n</answer>\n", 'role': 'assistant'}, {'content': "Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?", 'role': 'user'}, {'content': '<reasoning>\nShe spend $56 because 7 x 8 = <<7*8=56>>56\nShe has $44 left in the bank because 100 - 56 = <<100-56=44>>44\nShe can get 8 five dollar bills because 44 / 5 = <<44/5=8.8>>8.8\nThis is equal to $40 because 8 x 5 = <<8*5=40>>40\nShe has $4 left in the account because 44 - 40 = <<44-40=4>>4\n</reasoning>\n<answer>\n4\n</answer>\n', 'role': 'assistant'}, {'content': 'Ezekiel hikes as a hobby. This past summer, he did a challenging three-day hike across 50 kilometers of wilderness. The first day, he covered 10 kilometers of steep mountainside. The second day was flatter and he was able to cover half the full hike distance. How many kilometers did he have to hike on the third day to finish the hike?', 'role': 'user'}, {'content': '<reasoning>\nAfter the first day, Ezekiel had 50 - 10 = <<50-10=40>>40 kilometers of the hike left.\nOn the second day, he covered 50 / 2 = <<50/2=25>>25 kilometers.\nTherefore, on the third day, he had 40 - 25 = <<40-25=15>>15 kilometers left to finish the hike.\n</reasoning>\n<answer>\n15\n</answer>\n', 'role': 'assistant'}, {'content': "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?", 'role': 'user'}]
<|im_start|>system

Respond in the following format:
<reasoning>
...
</reasoning>
<answer>
...
</answer>
<|im_end|>
<|im_start|>user
Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|im_end|>
<|im_start|>assistant
<reasoning>
Mimi has 2 x 12 = <<2*12=24>>24 sea shells.
Kyle has 24 x 2 = <<24*2=48>>48 sea shells.
Leigh has 48 / 3 = <<48/3=16>>16 sea shells.
</reasoning>
<answer>
16
</answer>
<|im_end|>
<|im_start|>user
Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>
<|im_start|>assistant
<reasoning>
He has 6 - 2 = <<6-2=4>>4 cats.
He has 4 - 1 = <<4-1=3>>3 parrots.
He has 4 + 6 = <<4+6=10>>10 snakes.
He has a total of 2 + 4 + 3 + 10 = <<2+4+3+10=19>>19 pets.
</reasoning>
<answer>
19
</answer>
<|im_end|>
<|im_start|>user
Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?<|im_end|>
<|im_start|>assistant
<reasoning>
Dad gave Olaf 10 toy cars,
Mom has given Olaf 5 more toy cars than Dad, so 10 + 5 = <<10+5=15>>15 toy cars
Auntie gave Olaf 6 toy cars,
Uncle has given 1 less toy than Auntie, so 6 - 1 = <<6-1=5>>5 toy cars
Grandpa gave Olaf 2 * 5 = <<2*5=10>>10 toy cars.
All the family together gave Olaf 10 +15 + 6 + 5 + 10 = <<10+15+6+5+10=46>>46.
Adding the cars Olaf already had, Olaf's collection has 150 + 46 = <<150+46=196>>196 cars.
</reasoning>
<answer>
196
</answer>
<|im_end|>
<|im_start|>user
Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?<|im_end|>
<|im_start|>assistant
<reasoning>
She spend $56 because 7 x 8 = <<7*8=56>>56
She has $44 left in the bank because 100 - 56 = <<100-56=44>>44
She can get 8 five dollar bills because 44 / 5 = <<44/5=8.8>>8.8
This is equal to $40 because 8 x 5 = <<8*5=40>>40
She has $4 left in the account because 44 - 40 = <<44-40=4>>4
</reasoning>
<answer>
4
</answer>
<|im_end|>
<|im_start|>user
Ezekiel hikes as a hobby. This past summer, he did a challenging three-day hike across 50 kilometers of wilderness. The first day, he covered 10 kilometers of steep mountainside. The second day was flatter and he was able to cover half the full hike distance. How many kilometers did he have to hike on the third day to finish the hike?<|im_end|>
<|im_start|>assistant
<reasoning>
After the first day, Ezekiel had 50 - 10 = <<50-10=40>>40 kilometers of the hike left.
On the second day, he covered 50 / 2 = <<50/2=25>>25 kilometers.
Therefore, on the third day, he had 40 - 25 = <<40-25=15>>15 kilometers left to finish the hike.
</reasoning>
<answer>
15
</answer>
<|im_end|>
<|im_start|>user
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?<|im_end|>
<|im_start|>assistant

GSM8K Test Accuracy: 0.0000 (0/0)

Error Analysis:
Total errors: 0
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/josephL/efficient-icl/train_grpo.py", line 209, in <module>
[rank0]:     utils.analyze_errors(results)
[rank0]:   File "/home/josephL/efficient-icl/utils.py", line 236, in analyze_errors
[rank0]:     print(f"Numeric but wrong: {numeric_but_wrong} ({numeric_but_wrong/len(errors)*100:.1f}%)")
[rank0]:                                                      ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~
[rank0]: ZeroDivisionError: division by zero
[rank0]:[W329 02:29:16.930787321 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
