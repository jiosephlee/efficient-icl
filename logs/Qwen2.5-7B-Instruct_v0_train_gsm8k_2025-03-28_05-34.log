2025-03-28 05:34:52,055 - INFO - Starting experiment: Qwen2.5-7B-Instruct_v0_train_gsm8k_2025-03-28_05-34
2025-03-28 05:34:52,055 - INFO - Arguments: Namespace(model='Qwen/Qwen2.5-7B-Instruct', mode='train', lora_name='v0', dataset='gsm8k', checkpoint_for_continued_training=None)
2025-03-28 05:34:52,055 - INFO - Using max_seq_length=2048, lora_rank=32
2025-03-28 05:35:24,672 - INFO - Model loaded: Qwen/Qwen2.5-7B-Instruct
2025-03-28 05:35:27,862 - INFO - PEFT model configured
2025-03-28 05:35:27,862 - INFO - Loading gsm8k training dataset
2025-03-28 05:35:28,751 - INFO - Dataset loaded with 7473 examples
2025-03-28 05:35:28,792 - INFO - Training configuration: UnslothGRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.99,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.04,
bf16=False,
bf16_full_eval=False,
data_seed=3407,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
eval_accumulation_steps=2,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/Qwen2.5-7B-Instruct/v0/runs/Mar28_05-35-28_ShenLab,
logging_first_step=False,
logging_nan_inf_filter=False,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_completion_length=1536,
max_grad_norm=0.1,
max_prompt_length=512,
max_steps=250,
metric_for_best_model=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_generations=8,
num_train_epochs=3.0,
optim=OptimizerNames.PAGED_ADAMW_8BIT,
optim_args=None,
optim_target_modules=None,
output_dir=./checkpoints/Qwen2.5-7B-Instruct/v0/,
overwrite_output_dir=None,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.9,
ref_model_sync_steps=64,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=./checkpoints/Qwen2.5-7B-Instruct/v0/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=250,
save_strategy=SaveStrategy.STEPS,
save_total_limit=None,
seed=3407,
skip_memory_metrics=True,
split_batches=None,
sync_ref_model=False,
temperature=0.9,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=250,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
unsloth_num_chunks=-1,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_vllm=False,
vllm_device=auto,
vllm_dtype=auto,
vllm_gpu_memory_utilization=0.9,
vllm_max_model_len=None,
vllm_sampling_params=None,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.1,
)
2025-03-28 05:35:28,850 - INFO - Starting training
2025-03-28 10:40:26,442 - INFO - Training completed
2025-03-28 10:40:26,442 - INFO - Saving LoRA adapter to models/Qwen2.5-7B-Instruct/v0/
2025-03-28 10:40:32,380 - INFO - Output without LoRA:
2025-03-28 10:40:32,380 - INFO - Calculating pi to an exact value is not possible because pi is an irrational number, which means it has an infinite number of non-repeating digits. However, we can approximate pi to any degree of accuracy needed.

A common method to approximate pi is using the Monte Carlo method, which involves random sampling. Another method is the use of infinite series, such as the Leibniz formula for π:

\[
\pi = 4 \times (1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} - \cdots)
\]

This series converges very slowly, so many terms are needed for a good approximation.

For practical purposes, pi is often approximated as 3.14159 or 3.14 for simpler calculations.

To provide a more precise approximation, here is pi to 50 decimal places: 3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679.

<answer>
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679
</answer>
2025-03-28 10:40:36,860 - INFO - Output with LoRA:
2025-03-28 10:40:36,860 - INFO - Calculating pi to a high degree of accuracy is not something that can be done in a simple response due to the infinite non-repeating decimal places of pi. However, I can provide an approximation or discuss methods to calculate it.

For a basic approximation, pi is often taken as 3.14159. For more precision, it can be extended to 3.141592653589793.

To calculate pi, one common method is to use the Leibniz formula for π, which is an infinite series:
\[ \pi = 4 \times (1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} - \ldots) \]

This series converges very slowly, meaning you need to sum a large number of terms to get an accurate value. More efficient algorithms, such as the Chudnovsky algorithm, are used in practice for high-precision calculations.

For the purpose of this question, we can provide the value of pi to a reasonable number of decimal places.

<answer>
3.141592653589793
</answer>
2025-03-28 10:40:36,860 - INFO - Loading gsm8k test dataset for evaluation
2025-03-28 10:40:38,483 - INFO - Test dataset loaded with 1319 examples
2025-03-28 10:40:38,483 - INFO - Starting evaluation
2025-03-28 10:40:38,483 - INFO - Using the lora adapter: models/Qwen2.5-7B-Instruct/v0/
2025-03-28 12:07:50,742 - INFO - Detailed results saved to ./models/Qwen2.5-7B-Instruct/v0//gsm8k_2025-03-28_05-34_results.json
2025-03-28 12:07:50,742 - INFO - Accuracy: 0.79%
2025-03-28 12:07:50,742 - INFO - Experiment completed. Log saved to ./logs/Qwen2.5-7B-Instruct_v0_train_gsm8k_2025-03-28_05-34.log
